\documentclass[a4paper,10pt]{article}
\usepackage[margin=2cm]{geometry}
\usepackage[utf8]{inputenc}



\newcommand{\trnsp}{\mathsf{T}}
\usepackage[round]{natbib}
\renewcommand{\bibname}{References}
\bibliographystyle{abbrvnat}

\usepackage{amsmath, amsfonts}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{float}
\usepackage{hyperref}
\hypersetup{colorlinks=true, urlcolor=blue, citecolor=magenta}


\makeatletter
\@addtoreset{exercise}{section}
\makeatother  
\newcounter{exercise}
\newtheorem{exercise}{Exercise}
\renewcommand{\theexercise}{\arabic{exercise}}
\newsavebox{\mybox}
\newenvironment{note}
{
\begin{center}
\begin{lrbox}{\mybox}
\begin{minipage}{42em}}
{\end{minipage}
\end{lrbox}\fbox{\usebox{\mybox}}
\end{center}}

\newcommand{\R}{\mathbb{R}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\Var}{\text{Var}}

\title{Seminars on Overparametrized Machine Learning: \\Hand-in assignment 2}
\author{Antônio H. Ribeiro, Dave Zachariah, Per Mattsson}
\date{}

\begin{document}

\maketitle
\vspace{6pt}
\begin{center}
	\large \textbf{Due: 14th  of October 2021, 23:59}
\end{center}

\textit{
The items in the assignment can be implemented in the programming language of choice.
Nonetheless, we recommend the use of Python as a programming language, since we might include suggestions of functions in the exercise description.}



\section*{Double-descent in Linear Regression with Random Covariates}


You are going to reproduce the experiment in Figure 2 from~\citep{hastie_surprises_2019}. The instructions give some extra details on how to do it. You should obtain both empirical and asymptotic results and plot them together. You will use random i.i.d. covariates and estimate the parameters using the minimum-norm solution.

\paragraph{Data.} You will generate simulated data similar to the one described in~\citep{hastie_surprises_2019} in Section 3. The data will come from a linear model with isotropic features. 
\begin{itemize}
    \item \textbf{Isotropic inputs.} You will generate random inputs to be used to train and test your model. The $i$-th input will be a vector with size $p$:
    \[\textbf{x}_{i} = (x_{i1}, \cdots, x_{ip})\]
    where the entries $x_{ij}$ are i.i.d. with zero mean and variance 1 (for instance, values sampled from ${\mathcal{N}(0, 1)}$)).
    \item \textbf{Parameter vector with fixed $\ell_2$ norm.} Choose a parameter vector $\boldsymbol{\beta} \in \R^p$ to be used to generate the data. You can use any arbitrary value that satisfy the restriction $\|\boldsymbol{\beta}\|_2=r$ for the values of $r$ specified latter. As an example you could sample the entries of $\beta_j$ from the normal distribution $\mathcal{N}(0, \frac{r^2}{p})$. Or you could just make it constant $\boldsymbol{\beta} = \left(\frac{r}{\sqrt{p}}, \cdots, \frac{r}{\sqrt{p}}\right)$.
    \item \textbf{Linear model.} You should generate the dataset $(\textbf{x}_i, y_i) \in \R^p \times \R$ for $i=1, \dots, n$ according to the linear equations
    \begin{equation}
        y_i = \textbf{x}_i^\trnsp \boldsymbol{\beta} + \epsilon_i, \label{eq:data-model}
    \end{equation}
    for which the random draws across $i = 1, \dots, n$ are independent.  The error $\epsilon_i$ is generated independently from $\textbf{x}_i$ and comes from a distribution with zero mean and variance $\sigma^2$.
    \item \textbf{Train and test datasets.} Using this procedure generate both train and test datasets. We denote the training dataset using matrix notation $(\textbf{X}, \textbf{y})$ where $\textbf{X} \in \R^{n \times p} $ is the matrix containing the $j$-th entry of the $i$-th input $x_{ij}$ at position $(i, j)$ and $y \in \R^n$ is the vector of outputs. The training dataset should contain $n = 100$ samples. And, let the test set be denoted by $(\textbf{X}_{\text{test}}, \textbf{y}_{\text{test}})$ consist of $n_{\text{test}} = 100$  samples.
\end{itemize}



\paragraph{Model.} Estimated parameters by minimizing the sum of square errors
    \begin{equation}
         \label{eq:estimation-matrix}
        \frac{1}{n}\|\textbf{y} - \textbf{X} \boldsymbol{\beta}\|^2,  
    \end{equation}
    In the overparametrized regime, there are multiple solutions, and you should use the minimum $\ell_2$-norm solution to the problem (as used by \citet{hastie_surprises_2019,belkin_reconciling_2019}), i.e.:
    \begin{equation}
    \label{eq:min-norm-solution}
    \hat{\boldsymbol{\beta}} = \text{arg}\min_{\boldsymbol{\beta}} \|\boldsymbol{\beta}\|_2 \quad \text{subject to}\quad\textbf{y} = \textbf{X} \boldsymbol{\beta},
    \end{equation}
    This behaviour can be expressed by the analytical solution
    \begin{equation}
    \label{eq:ls-sol}
    \hat{\boldsymbol{\beta}} = ( \textbf{X}^\trnsp  \textbf{X})^{+} \textbf{X}^\trnsp \textbf{y},
    \end{equation}
    where $(\textbf{X}^\trnsp  \textbf{X})^{+}$ denotes the Moore-Penrose pseudo-inverse of $\textbf{X}^\trnsp  \textbf{X}$ which does yield the desired behaviour in both the underparametrized and overparametrized regions.  
    
    \begin{note}
    \textbf{Note:} \href{https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.lstsq.html}{scipy.linalg.lstsq} does yield the desired behaviour both in the underparametrized and overparametrized region.
    \end{note}
    
    \paragraph{Empirical evaluation.}  Evaluate your model on the test set by computing the mean square error
    \begin{equation}
         \label{eq:estimation-matrix}
        \text{MSE}_{\text{test}}  = \frac{1}{n_{\text{test}}}\|\textbf{y}_{\text{test}} - \textbf{X}_{\text{test}} \hat{\boldsymbol{\beta}}\|^2.
    \end{equation}
    
    \paragraph{Asymptotics} Use the asymptotics obtained by \citet{hastie_surprises_2019} in Theorem 1 and Coroloary 1, as $n, p\rightarrow \gamma$ and $p / n\rightarrow \gamma$,
    \begin{eqnarray}
        \label{eq:asymptotics-hastie}
        \text{MSE}_{\text{test}}  &\rightarrow&
        \begin{cases}
        \sigma^2 \frac{\gamma}{1 - \gamma} + \sigma^2& \gamma < 1, \\
        r^2 (1 - \frac{1}{\gamma}) + \sigma^2 \frac{1}{\gamma - 1}+ \sigma^2 & \gamma > 1,
        \end{cases}\\
        \|\hat{\boldsymbol{\beta}}\|_2^2 &\rightarrow&
        \begin{cases}
        r^2 + \sigma^2 \frac{\gamma}{1 - \gamma} & \gamma < 1, \\
        r^2 \frac{1}{\gamma} + \sigma^2 \frac{1}{\gamma - 1} &  \gamma > 1.
        \end{cases}
    \end{eqnarray}
    \begin{note}
    \begin{itemize}
    \item \textbf{Note 1:}
    More precisely, \citet{hastie_surprises_2019} establishes that the expectation (conditioned on $\textbf{X}$) of the values on the left hand side converge, almost surely, to the values on the right hand side  under the appropriate conditions.
    \item \textbf{Note 2:}
    You might notice some subtle differences in the formulation above from the one in~\citet{hastie_surprises_2019}. They give the asymptotics for the risk $R = E[\textbf{x}_0^\trnsp\hat{\boldsymbol{\beta}} -  \textbf{x}_0^\trnsp\boldsymbol{\beta}]$, where $\textbf{x}_0$ is a test point not seem during training. Here, we work directly with the mean square error on the test set instead.  Eq.~\ref{eq:asymptotics-hastie} above follows from the fact that $\text{MSE}_{\text{test}}\rightarrow R + \sigma^2$ as $n_{\text{test}} \rightarrow \infty$.
    \end{itemize}
    \end{note}
\section*{Exercise}
Choose $\sigma^2$ and $r^2$, and  (for the values you have choosen) plot the \textbf{asymptotics} \textit{and, also,} the \textbf{empirical values} of
\begin{enumerate}
    \item The \textbf{test} mean square error;
    \item  The \textbf{parameter $\ell_2$ norm} $\|\beta\|_2$.
\end{enumerate}


For the \textbf{empirical evaluation}: fix the number of training points $n = 200$, and perform the empirical evaluation described above. Do it for the number of parameters $p$ ranging from $0.1n$ to $10 n$ (generate at least 100 configurations in this range using logspace). Plot as a function of the ratio $\gamma = p / n$.

For the \textbf{asymptotics} evaluation: Compute the asymptotic values for $\gamma$ ranging from $0.1$ to $10$ (use a finer grid so it looks continuous). Plot the asymptotics superimposed to the empirically obtained values.

   \begin{note}
    Some tips for the plots to look nice:
    \begin{itemize}
        \item Close to the interpolation point the test error takes very
        large values. We suggest to manually set the y-limits in the plot, so the region of interest is highlighted. The same also applies to the parameter norm.
        \item Using logscale in the x-axis might also make the plot more clear.
        \item Add a vertical line in the threshold $\gamma = 1$ to show where the interpolation threshold is.
        \item Use markers to plot the empirical results and lines to plot the asymptotics.
    \end{itemize}
    \end{note}

\section*{The Submission}
Your submission should have a single page of content (a4paper, fontsize=10pt, margin=2cm, both single and double column are acceptable...).  Include your name, the plots, a short description of the experiment parameters that you used and a paragraph of discussion/conclusion. You can assume that whoever will read your report has both read paper from~\citep{hastie_surprises_2019} and the entire description above, so there is no need for repeating it... 

All requested plots should have proper figure captions, legends, and axis labels. You should submit two files, one pdf-file with the report and a standalone script (or jupyter notebook) that can be used to run the code and generate the plots (Write as comments the packages/libraries versions and additional requirements as comments in the top of the script). Compress the two files as a single zip (containing pdf + script) and mail it to \href{mailto:antonio.horta.ribeiro@it.uu.se}{antonio.horta.ribeiro@it.uu.se}. You will receive a confirmation mail back.

% \bibliography{refs}
\begin{thebibliography}{2}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Belkin et~al.(2019)Belkin, Hsu, Ma, and
  Mandal]{belkin_reconciling_2019}
M.~Belkin, D.~Hsu, S.~Ma, and S.~Mandal.
\newblock Reconciling modern machine-learning practice and the classical
  bias–variance trade-off.
\newblock \emph{Proceedings of the National Academy of Sciences}, 116\penalty0
  (32):\penalty0 15849--15854, Aug. 2019.
\newblock ISSN 0027-8424, 1091-6490.
\newblock \doi{10.1073/pnas.1903070116}.

\bibitem[Hastie et~al.(2019)Hastie, Montanari, Rosset, and
  Tibshirani]{hastie_surprises_2019}
T.~Hastie, A.~Montanari, S.~Rosset, and R.~J. Tibshirani.
\newblock Surprises in {High}-{Dimensional} {Ridgeless} {Least} {Squares}
  {Interpolation}.
\newblock \emph{arXiv:1903.08560}, Nov. 2019.
\newblock URL \url{http://arxiv.org/abs/1903.08560}.

\end{thebibliography}

\end{document}
